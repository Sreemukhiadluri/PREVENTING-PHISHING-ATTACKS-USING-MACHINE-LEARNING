import pandas as pd import numpy as np import seaborn
as sns import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline') # It
sets the backend of matplotlib to the 'inline' backend:
import time # calculate time

from sklearn.linear_model import LogisticRegression from
sklearn.naive_bayes import MultinomialNB from sklearn.model_selection
import train_test_split # spliting the data between feature and target from
sklearn.metrics import classification_report # gives whole report about
metrics (e.g, recall,precision,f1_score,c_m) from sklearn.metrics import
confusion_matrix # gives info about actual and predict from nltk.tokenize
import RegexpTokenizer # regexp tokenizers use to split words from text
from nltk.stem.snowball import SnowballStemmer # stemmes words from
sklearn.feature_extraction.text import CountVectorizer # create sparse

“Preventing Phishing attacks”

Preventing Phishing Attacks Page No:28
matrix of words using regexptokenizes from sklearn.pipeline import
make_pipeline

from PIL import Image # getting images in notebook
# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from bs4 import BeautifulSoup # use for scraping the data from website from
selenium import webdriver # use for automation chrome import networkx as
nx # for the creation, manipulation, and study of the structure, dynamics, and
functions of complex networks.

import pickle# use to dump model

import warnings
warnings.filterwarnings('ignore')

phishing_data1 =
pd.read_csv("E:\ml\phishing_site_urls.csv",usecols=['URL','Label'],encoding=
'latin1', error_bad_lines=False)
phishing_data1.columns = ['URL','Label']

for l in range(len(phishing_data1.Label)<=300000):
if phishing_data1.Label.loc[l] == '1.0':
phishing_data1.Label.loc[l] = 'bad'
else:
phishing_data1.Label.loc[l] = 'good'

“Preventing Phishing attacks”

Preventing Phishing Attacks Page No:29
phish_data = pd.read_csv("E:\ml\phishing_site_urls.csv")
phish_data.head()

phish_data.tail()

phish_data.isnull().sum()# there is no missing values
#create a dataframe of classes counts
label_counts = pd.DataFrame(phish_data.Label.value_counts())

sns.set_style('darkgrid')
sns.barplot(label_counts.index,label_counts.Label
tokenizer = RegexpTokenizer(r'[A-Za-z]+')
phish_data.URL[0]
tokenizer.tokenize(phish_data.URL[0]) # using first row
print('Getting words tokenized ...') t0= time.perf_counter()
phish_data['text_tokenized'] = phish_data.URL.map(lambda t:
tokenizer.tokenize(t)) # doing with all rows t1 = time.perf_counter() - t0
print('Time taken',t1 ,'sec') phish_data.sample(9) stemmer =
SnowballStemmer("english") # choose a language print('Getting words
stemmed ...') t0= time.perf_counter() phish_data['text_stemmed'] =
phish_data['text_tokenized'].map(lambda l:
[stemmer.stem(word) for word in l])
t1= time.perf_counter() - t0
print('Time taken',t1 ,'sec')

“Preventing Phishing attacks”

Preventing Phishing Attacks Page No:30
phish_data.sample(9) print('Getting joiningwords ...') t0= time.perf_counter()
phish_data['text_sent'] = phish_data['text_stemmed'].map(lambda l: ' '.join(l))
t1= time.perf_counter() - t0 print('Time taken',t1 ,'sec') phish_data.sample(9)
#sliceing classes bad_sites = phish_data[phish_data.Label == 'bad']
good_sites = phish_data[phish_data.Label == 'good'] bad_sites.head()
good_sites.head() from nltk.corpus import stopwords browser =
webdriver.Chrome(r"chromedriver.exe") list_urls =
['https://www.tutorialspoint.com/questions/index.php'] #here iam take
phishing sites links_with_text = [] #beautiful soup for url in list_urls:
browser.get(url) soup = BeautifulSoup(browser.page_source,"html.parser")
for line in soup.find_all('a'):
href = line.get('href') links_with_text.append([url,
href])
df = pd.DataFrame(links_with_text, columns=["from", "to"]) df.head()
#create cv object cv = CountVectorizer() #help(CountVectorizer()) feature =
cv.fit_transform(phish_data.text_sent) #transform all text which we
tokenize and stemed feature[:5].toarray() # convert sparse matrix into array
to print transformed features trainX, testX, trainY, testY =
train_test_split(feature, phish_data.Label)
# create lr object lr = LogisticRegression() lr.fit(trainX,trainY)
lr.score(testX,testY) print('Training Accuracy
:',lr.score(trainX,trainY)) print('Testing Accuracy
:',lr.score(testX,testY)) con_mat =
pd.DataFrame(confusion_matrix(lr.predict(testX), testY), columns =
['Predicted:Bad', 'Predicted:Good'], index = ['Actual:Bad',
'Actual:Good'])

print('\nCLASSIFICATION REPORT\n')
print(classification_report(lr.predict(testX), testY, target_names

“Preventing Phishing attacks”

Preventing Phishing Attacks Page No:31
=['Bad','Good']))

print('\nCONFUSION MATRIX')
plt.figure(figsize= (6,4)) sns.heatmap(con_mat, annot =
True,fmt='d',cmap="YlGnBu") pipeline_ls =
make_pipeline(CountVectorizer(tokenizer =

RegexpTokenizer(r'[A-Za-
z]+').tokenize,stop_words='english'),

LogisticRegression())
##(r'\b(?:http|ftp)s?://\S*\w|\w+|[^\w\s]+') ([a-zA-Z]+)([0-9]+) -- these
tolenizers giving me low accuray trainX, testX, trainY, testY =
train_test_split(phish_data.URL, phish_data.Label)
pipeline_ls.fit(trainX,trainY) pipeline_ls.score(testX,testY)
print('Training Accuracy :',pipeline_ls.score(trainX,trainY)) print('Testing
Accuracy :',pipeline_ls.score(testX,testY)) con_mat =
pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),
columns = ['Predicted:Bad', 'Predicted:Good'], index = ['Actual:Bad',
'Actual:Good'])

print('\nCLASSIFICATION REPORT\n')
print(classification_report(pipeline_ls.predict(testX), testY, target_names
=['Bad','Good']))

print('\nCONFUSION MATRIX')
plt.figure(figsize= (6,4)) sns.heatmap(con_mat, annot =
True,fmt='d',cmap="YlGnBu") filename_pkl = 'finalized_model.sav'
pickle.dump(pipeline_ls, open(filename_pkl, 'wb')) loaded_model = pickle.
load(open(filename_pkl, 'rb')) result = loaded_model. score(testX, testY)

“Preventing Phishing attacks”

Preventing Phishing Attacks Page No:32

print(result) predict_bad = ['yeniik.com.tr/wp-
admin/js/login.alibaba.com/login.jsp.php','fazan-
pacir.rs/temp/libraries/ipad','tubemoviez.exe','svision-
online.de/mgfi/administrator/components/com_babackup/classes/fx29id1.txt'

] predict_good =
['youtube.com/','youtube.com/watch?v=qI0TQJI3vdU','retailhellunderground.
c om/','restorevisioncenters.com/html/technology.html'] loaded_model =
pickle.load(open('phishing.pkl', 'rb')) #predict_bad =
vectorizers.transform(predict_bad) # predict_good =
vectorizer.transform(predict_good) result =
loaded_model.predict(predict_bad) result2 =
loaded_model.predict(predict_good)
print(result) print(result2)
pickle.dump(pipeline_ls,open('phishing.pkl','wb'))
loaded_model = pickle.load(open('phishing.pkl', 'rb'))
result = loaded_model.score(testX,testY) print(result)